# Kyle Savell, Henry Wheeler-Mackta, Richard Valente

from keras.models import Sequential
from keras.layers import Dense, Activation
from sklearn.model_selection import train_test_split
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import numpy as np
import itertools

# Symbolic Constants
K_LOW = 5
K_HIGH = 10


# Helper functions for feature creation
def get_pixel_features(num_array):
    """
    Gets features for a given picture pixel array
    :param num_array:
    :return: an array of the features
    """
    array_length = len(num_array)
    array_depth = len(num_array[0])
    pixel_counter = 0
    blank_pixel_counter = 0
    highest_pixel_location = 999
    lowest_pixel_location = 0
    leftist_pixel_location = 999
    rightmost_pixel_location = 0

    for j in range(array_length):
        for k in range(array_depth):
            if num_array[j][k] > 0:
                pixel_counter += 1
                if k < highest_pixel_location:
                    highest_pixel_location = k
                if k > lowest_pixel_location:
                    lowest_pixel_location = k
                if j < leftist_pixel_location:
                    leftist_pixel_location = j
                if j > rightmost_pixel_location:
                    rightmost_pixel_location = j

            if num_array[j][k] == 0:
                blank_pixel_counter += 1

    return_array = [pixel_counter / 28*28, blank_pixel_counter, highest_pixel_location, lowest_pixel_location, leftist_pixel_location, rightmost_pixel_location]
    print(return_array)
    return return_array


# Load and Flatten Data
all_labels = np.load('labels.npy')  # Load in labels, these being the "target"
all_images = np.load('images.npy')  # Load in images, these being the "data"
flat_images = []
features = []

for image in all_images:
    temp = image.ravel()
    flat_images.append(temp)
    # append to additional feature arrays
    extra_features = get_pixel_features(image)
    features.append(extra_features)

print(len(features))
# Create Training, Validation and Test Sets
training_set_i, validation_set_i, test_set_i = [], [], []  # Image sets
training_set_f, validation_set_f, test_set_f = [], [], []  # custom feature sets
training_set_l, validation_set_l, test_set_l = [], [], []  # Label sets
i = 0

while i < 3900:
    training_set_i.append(flat_images[i])
    training_set_f.append(features[i])
    training_set_l.append(all_labels[i])
    i += 1
while i < 4875:
    validation_set_i.append(flat_images[i])
    validation_set_f.append(features[i])
    validation_set_l.append(all_labels[i])
    i += 1
while i < 6500:
    test_set_i.append(flat_images[i])
    test_set_f.append(features[i])
    test_set_l.append(all_labels[i])
    i += 1

predicted_labels = []  # Set of predicted labels generated by the algorithms


# Class for the Decision Tree Classification
class DecisionTreeClassification:
    def __init__(self, data, target, tree):
        """
        DecisionTreeClassification takes an array of data
        :param data: the training data
        :param target: the training target
        :param: tree: the DecisionTreeClassifier
        """
        self.data = data
        self.target = target
        self.estimator = tree

    def fit(self):
        """
        Builds the decision tree using scikit's decision tree classifiers
        :return: the classified tree
        """
        self.estimator.fit(self.data, self.target)
        return self.estimator

    def score_accuracy(self, x_input, y_input):
        """
        Returns the accuracy score for the tree given some images and labels
        :param x_input: array of inputs (in our case, the pictures)
        :param y_input: truth array (in our case, the labels)
        :return: the accuracy score
        """
        return self.estimator.score(x_input, y_input)

    def predict(self, x_test):
        """
        Predicts based on some test input and prints how it stacks up against truth
        Please calla after "classify"
        :param x_test: the test array of inputs (in our case, the pictures)
        :return: the result
        """
        predict = self.estimator.predict(x_test)
        return predict


# Function from scikit-learn documentation:
# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

        """
        This function prints and plots the confusion matrix.
        Normalization can be applied by setting `normalize=True`.
        """
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print("Normalized confusion matrix")
        else:
            print('Confusion matrix, without normalization')

        print(cm)

        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
        plt.colorbar()
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)

        fmt = '.2f' if normalize else 'd'
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fmt),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')


# K-Nearest Neighbors
def k_nearest():
    cur_k = K_LOW
    best_k = 0
    best_sum = 0

    # Find best k value using validation set
    while cur_k <= K_HIGH:
        neigh = KNeighborsClassifier(n_neighbors=cur_k)
        neigh.fit(training_set_i, training_set_l)
        predicted_data = neigh.predict(validation_set_i)
        cf = confusion_matrix(validation_set_l, predicted_data)
        print(cf)
        cur_val = 0
        sum_correct = 0
        while cur_val < 10:
            sum_correct += cf[cur_val][cur_val]
            cur_val += 1
        if sum_correct > best_sum:
            best_sum = sum_correct
            best_k = cur_k
        cur_k += 1

    # Use best k on test set
    print("Best k-value is "+str(best_k))
    neigh = KNeighborsClassifier(n_neighbors=cur_k)
    neigh.fit(training_set_i, training_set_l)
    predicted_data = neigh.predict(test_set_i)
    cf = confusion_matrix(test_set_l, predicted_data)
    print(cf)
    return cf


# run the program
def main():
    print("Starting...")

    # Decision Tree
    # DecisionTreeClassifier(criterion=’gini’, splitter =’best’, max_depth = None, min_samples_split = 2,
    #                       min_samples_leaf = 1, min_weight_fraction_leaf = 0.0, max_features = None,
    #                       random_state = None, max_leaf_nodes = None, min_impurity_decrease = 0.0,
    #                       min_impurity_split = None, class_weight = None, presort = False)
    print("Decision Tree Working on Full Pixel Data: ")
    dt = DecisionTreeClassifier(max_depth=19, random_state=0, min_samples_leaf=1, )
    dt = DecisionTreeClassification(training_set_i, training_set_l, dt)
    dt.fit()

    print('Accuracy on the training subset: {:.3f}'.format(dt.score_accuracy(training_set_i, training_set_l)))
    print('Accuracy on the validation subset: {:.3f}'.format(dt.score_accuracy(validation_set_i, validation_set_l)))
    print('Accuracy on the test subset: {:.3f}'.format(dt.score_accuracy(test_set_i, test_set_l)))
    print('Accuracy score on prediction of test subset:{:.3f}'.format(accuracy_score(training_set_l,
                                                                      dt.predict(training_set_i))))
    print('Confusion matrix on validation subset:')
    print(confusion_matrix(validation_set_l, dt.predict(validation_set_i)))

    print("Decision Tree Working on Hand-Engineered Features: ")
    dt2 = DecisionTreeClassifier(max_depth=19, random_state=0, min_samples_leaf=1, )
    dt2 = DecisionTreeClassification(training_set_f, training_set_l, dt2)
    dt2.fit()

    print('Accuracy on the training hand-engineered feature subset: {:.3f}'.format(dt2.score_accuracy(training_set_f,
                                                                                                     training_set_l)))
    print('Accuracy on the validation hand feature subset: {:.3f}'.format(dt2.score_accuracy(validation_set_f,
                                                                                            validation_set_l)))
    print('Accuracy on the test hand-engineered feature subset: {:.3f}'.format(dt2.score_accuracy(test_set_f,
                                                                                                 test_set_l)))
    print('Accuracy score on prediction of test hand feature subset:{:.3f}'.format(accuracy_score(training_set_l,
                                                                                            dt2.predict(training_set_f))))
    print('Confusion matrix on test hand-engineered feature subset:')
    print(confusion_matrix(validation_set_l, dt2.predict(validation_set_f)))

    # K-Nearest Neighbors
    cf = k_nearest()
    class_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    # Plot non-normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cf, classes=class_labels,
                          title='Confusion matrix, without normalization')

    # Plot normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cf, classes=class_labels, normalize=True,
                          title='Normalized confusion matrix')

    plt.show()


main()
