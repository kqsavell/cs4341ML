# Kyle Savell, Henry Wheeler-Mackta, Richard Valente

from keras.models import Sequential
from keras.layers import Dense, Activation
from sklearn.model_selection import train_test_split
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Symbolic Constants
K_LOW = 5
K_HIGH = 10

# Load and Flatten Data
all_labels = np.load('labels.npy')  # Load in labels, these being the "target"
all_images = np.load('images.npy')  # Load in images, these being the "data"
flat_images = []

for image in all_images:
    temp = image.ravel()
    flat_images.append(temp)

    # Create Training, Validation and Test Sets
    training_set_i, validation_set_i, test_set_i = [], [], []  # Image sets
    training_set_l, validation_set_l, test_set_l = [], [], []  # Label sets
    i = 0
    while i < 3900:
        training_set_i.append(flat_images[i])
        training_set_l.append(all_labels[i])
        i += 1
    while i < 4875:
        validation_set_i.append(flat_images[i])
        validation_set_l.append(all_labels[i])
        i += 1
    while i < 6500:
        test_set_i.append(flat_images[i])
        test_set_l.append(all_labels[i])
        i += 1

predicted_labels = []  # Set of predicted labels generated by the algorithms


# Class for the Decision Tree Classification
class DecisionTreeClassification:
    def __init__(self, data, target, tree):
        """
        DecisionTreeClassification takes an array of data
        :param data: the training data
        :param target: the training target
        :param: tree: the DecisionTreeClassifier
        """
        self.data = data
        self.target = target
        self.estimator = tree

    def fit(self):
        """
        Builds the decision tree using scikit's decision tree classifiers
        :return: the classified tree
        """
        self.estimator.fit(self.data, self.target)
        return self.estimator

    def score_accuracy(self, x_input, y_input):
        """
        Returns the accuracy score for the tree given some images and labels
        :param x_input: array of inputs (in our case, the pictures)
        :param y_input: truth array (in our case, the labels)
        :return: the accuracy score
        """
        return self.estimator.score(x_input, y_input)

    def predict(self, x_test):
        """
        Predicts based on some test input and prints how it stacks up against truth
        Please calla after "classify"
        :param x_test: the test array of inputs (in our case, the pictures)
        :return: the result
        """
        predict = self.estimator.predict(x_test)
        return predict


# K-Nearest Neighbors
def k_nearest():
    cur_k = K_LOW
    best_k = 0
    best_sum = 0

    # Find best k value using validation set
    while cur_k <= K_HIGH:
        neigh = KNeighborsClassifier(n_neighbors=cur_k)
        neigh.fit(training_set_i, training_set_l)
        predicted_data = neigh.predict(validation_set_i)
        cf = confusion_matrix(validation_set_l, predicted_data)
        print(cf)
        cur_val = 0
        sum_correct = 0
        while cur_val < 10:
            sum_correct += cf[cur_val][cur_val]
            cur_val += 1
        if sum_correct > best_sum:
            best_sum = sum_correct
            best_k = cur_k
        cur_k += 1

    # Use best k on test set
    print("Best k-value is "+str(best_k))
    neigh = KNeighborsClassifier(n_neighbors=cur_k)
    neigh.fit(training_set_i, training_set_l)
    predicted_data = neigh.predict(test_set_i)
    cf = confusion_matrix(test_set_l, predicted_data)
    print(cf)
    return cf


# run the program
def main():
    print("Starting...")

    # Decision Tree
    # DecisionTreeClassifier(criterion=’gini’, splitter =’best’, max_depth = None, min_samples_split = 2,
    #                       min_samples_leaf = 1, min_weight_fraction_leaf = 0.0, max_features = None,
    #                       random_state = None, max_leaf_nodes = None, min_impurity_decrease = 0.0,
    #                       min_impurity_split = None, class_weight = None, presort = False)
    dt = DecisionTreeClassifier(max_depth=5,random_state=0)
    dt = DecisionTreeClassification(training_set_i, training_set_l, dt)
    dt.fit()
    print('Accuracy on the training subset: {:.3f}'.format(dt.score_accuracy(training_set_i, training_set_l)))
    print('Accuracy on the validation subset: {:.3f}'.format(dt.score_accuracy(validation_set_i, validation_set_l)))
    print('Accuracy on the test subset: {:.3f}'.format(dt.score_accuracy(test_set_i, test_set_l)))
    print('Accuracy score on prediction of test subset:{:.3f}'.format(accuracy_score(training_set_l),
                                                                      dt.predict(training_set_i)))
    print('Confusion matrix on test subset:')
    print(confusion_matrix(training_set_l, dt.predict(training_set_i)))

    # K-Nearest Neighbors
    k_nearest()


main()
